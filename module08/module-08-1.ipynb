{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAT303 Module 8.1 Notebook\n",
    "---\n",
    "Name:    \n",
    "Date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is assumed you are using the module8 conda environment specified in the *module8.yaml* file downloaded from Canvas. Be sure to read all cells in this notebook. You are only to provide code in cells that contain `##### YOUR CODE HERE #####` and written responses in cells that contain `YOUR WRITTEN RESPONSE HERE`. Ensure that code cells are executed sequentially to prevent unexpected errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will work with the CIFAR-10 dataset to train a convolutional neural network in order to classify input images into one of the 10 pre-defined classes (more information on CIFAR-10 is available [here](https://dmacc.instructure.com/courses/11337/pages/the-cifar-10-dataset)). \n",
    "\n",
    "This notebook differs from previous modules in that you are not expected to do as much coding: The code to load the data, define the model, initialize the optimizer and setup the training and validation loops has been provided. You will then re-use this existing code and assess how adjusting various parameters such as dropout, learning rate and momentum affect the model's performance. Note that training on CPU may take some time: Training BasicCNN for 25 epochs via CPU requires anywhere from 5-15 minutes. \n",
    "\n",
    "\n",
    "* Part I: Evaluate BasicCNN with default settings.\n",
    "* Part II: Evaluate BasicCNN with different combinations of learning rate, dropout and momentum.\n",
    "\n",
    "<br>\n",
    "\n",
    "**BE SURE TO READ THE INSTRUCTIONS FOR ALL SECTIONS!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Part I.\n",
    "\n",
    "\n",
    "1. Start by loading in the CIFAR-10 dataset. The next cell loads and normalizes the images and creates `train_loader` and `valid_loader` instances, which are iterated over during training. Execute the cell below (no additional code required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=8, linewidth=1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "\n",
    "# Number of images to process in each batch. \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Specify CIFAR-10 classes.\n",
    "classes = [\n",
    "    \"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \n",
    "    \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# ImageNet transforms to normalize images. \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "     )\n",
    "\n",
    "\n",
    "# Download CIFAR-10 training and validation data.\n",
    "train_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "valid_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "# Create training and validation DataLoader instances. This is what gets \n",
    "# iterated over during training. \n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nNumber of training batches of size {batch_size}: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. Display a batch of images to get an idea of what the training data looks like. Execute the cell below (no additional code required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get first batch from train loader. \n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 9), tight_layout=True)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"CIFAR-10 Training Images\", fontsize=12)\n",
    "ax.imshow(np.transpose(torchvision.utils.make_grid(images[:64], padding=1, normalize=True), (1, 2, 0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Define the network architecture. Refer to the *Anatomy of a PyTorch Neural Network* page in Canvas for a description of each component of the model. Prints the total number of trainable parameters. Execute the cell below (no additional code required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, dropout=0):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.drp = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = self.pool(F.relu(self.conv1(X)))\n",
    "        output = self.pool(F.relu(self.conv2(output)))\n",
    "        output = torch.flatten(output, 1)\n",
    "        output = F.relu(self.drp(self.fc1(output)))\n",
    "        output = F.relu(self.drp(self.fc2(output)))\n",
    "        output = self.fc3(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Print number of trainable parameters.\n",
    "nbr_params = sum(p.numel() for p in BasicCNN().parameters())\n",
    "print(f\"BasicCNN nbr_params: {nbr_params:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. Define the loss function and optimizer. For the loss function we use cross entropy loss and stochastic gradient descent as the optimizer. We also initialize an instance of our model, passing it the specified value for dropout. For the first run dropout will be set to 0. You'll have an opportunity to evaluate different values for dropout later in the notebook. Execute the cell below (no additional code required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Configuration ----------------------------------------------------------------\n",
    "\n",
    "# Number of epochs.\n",
    "n_epochs = 25\n",
    "\n",
    "# Learning rate.\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum.\n",
    "momentum = .90\n",
    "\n",
    "# Dropout.\n",
    "dropout = 0.0\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Check if gpu is available. If not, use cpu. \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize instance of BasicCNN. Put on device for completeness.\n",
    "mdl = BasicCNN(dropout=dropout).to(device)\n",
    "\n",
    "# Specify loss function.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify optimizer. \n",
    "optimizer = optim.SGD(mdl.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Define training and validation functions. One epoch is a full pass through the training data. For example, if we had 1,000 training images and used a batch size of 50, one epoch would consist of evaluating 1000 / 50 = 20 batches of images. We usually train a network for multiple epochs. If we trained the model for 10 epochs, we would pass the full training dataset through the model 10 times.  \n",
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model's parameters. Execute the cell below (no additional code required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def epoch_trainer(epoch, data_loader, model, loss_fn, optimizer, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Execute a single training epoch. Return last batch training loss\n",
    "    and accuracy. \n",
    "    \"\"\"\n",
    "    loss, checkpoint_loss, correct, samples = 0.0, 0.0, 0, 0\n",
    "\n",
    "    # Put model in train mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over batches in data_loader. \n",
    "    for ii, (X, yactual) in enumerate(data_loader):\n",
    "\n",
    "        # Send datasets to device. \n",
    "        X, yactual = X.to(device), yactual.to(device)\n",
    "\n",
    "        # Zero out parameter gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get model predictions (forward pass). \n",
    "        ypred = model(X)\n",
    "\n",
    "        # Compute loss. \n",
    "        loss_ii = loss_fn(ypred, yactual)\n",
    "\n",
    "        # Backpropagation and optimizer step. \n",
    "        loss_ii.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running_loss.\n",
    "        loss+=loss_ii.item()\n",
    "        correct+=(ypred.argmax(dim=1)==yactual).type(torch.float).sum().item()\n",
    "        samples+=yactual.size(dim=0)\n",
    "\n",
    "        # Print running_loss for every 100 mini-batches.\n",
    "        if ii % 250 == 0:\n",
    "            checkpoint_acc = correct / samples\n",
    "            checkpoint_loss = loss / 250\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\t+ [train][epoch={epoch}, batch={ii}] loss = {checkpoint_loss:,.5f}, acc = {checkpoint_acc:.5f}.\")\n",
    "            \n",
    "            loss, correct, samples = 0.0, 0, 0\n",
    "\n",
    "    return checkpoint_loss, checkpoint_acc\n",
    "        \n",
    "\n",
    "\n",
    "def epoch_validator(data_loader, model, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Execute a single validation epoch. Return average validation loss\n",
    "    and accuracy.\n",
    "    \"\"\"\n",
    "    valid_loss, correct = 0.0, 0\n",
    "\n",
    "    # Put model in validation mode.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for ii, (X, yactual) in enumerate(data_loader, start=1):\n",
    "\n",
    "            # Send dataset and target to device. \n",
    "            X, yactual = X.to(device), yactual.to(device)\n",
    "\n",
    "            # Get model predictions. \n",
    "            ypred = model(X)\n",
    "\n",
    "            # Compute loss and update valid_loss.\n",
    "            valid_loss+=loss_fn(ypred, yactual).item()\n",
    "\n",
    "            # Count number of correct class predictions.\n",
    "            correct+=(ypred.argmax(dim=1)==yactual).type(torch.float).sum().item()\n",
    "\n",
    "    loss, acc = valid_loss / ii, correct / len(data_loader.dataset)\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "6. Train model for specified number of epochs. `results` is a list of 4-tuples consisting of train loss, train accuracy, validation loss and validation accuracy. Note for this dataset, any accuracy above 10% shows that the model has learned something, since 10% is the accuracy of a model that selects classes randomly. Execute the cell below (no additional code required). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    tloss, tacc = epoch_trainer(\n",
    "        epoch=epoch, data_loader=train_loader, model=mdl, loss_fn=loss_fn, \n",
    "        optimizer=optimizer, device=device\n",
    "        )\n",
    "    \n",
    "    vloss, vacc = epoch_validator(\n",
    "        data_loader=valid_loader, model=mdl, loss_fn=loss_fn, \n",
    "        optimizer=optimizer, device=device\n",
    "        )\n",
    "    \n",
    "    print(f\"[epoch={epoch}]: tloss={tloss:.5f}, tacc={tacc:.5f}, vloss={vloss:.5f}, vacc={vacc:.5f}.\")\n",
    "\n",
    "    # Append metrics to results.\n",
    "    results.append((tloss, tacc, vloss, vacc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "7. Which epoch achieved the minimum validation loss? The maximum validation accuracy? What are the values of the minimum validation loss and maximum validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "8. Create a side-by-side plot with training and validation loss on the left and training and validation accuracy on the right. Be sure to include a legend and label you axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "9. Inspecting the graph, at which epoch do train and validation loss start to diverge? At which epoch to train and validation accuracy start to diverge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Part II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Re-run the same training pipeline above, but this time using different values for learning rate, momentum and dropout. For each set parameters, train the model for 25 epochs, and record the minimum validation loss and maximum validation accuracy for each parameter set. Note that for each set of parameters, you will need to re-initialize both the `BasicCNN` instance (since this takes dropout as a parameter) as well as the optimizer (since this takes learning rate and momentum as parameters). The `params` list of dicts provided in the next cell contains the full set of parameter combinations you are to evaluate.   \n",
    "Note that this step make take a while (possibly longer than an hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameter combinations to evaluate for ablation test.\n",
    "params = [\n",
    "    {\"lr\": .001, \"momentum\": 0.90, \"dropout\": .10},\n",
    "    {\"lr\": .001, \"momentum\": 0.95, \"dropout\": .05},\n",
    "    {\"lr\": .0005, \"momentum\": 0.90, \"dropout\": .10},\n",
    "    {\"lr\": .0005, \"momentum\": 0.95, \"dropout\": .05},\n",
    "    {\"lr\": .001, \"momentum\": 0.875, \"dropout\": .10},\n",
    "    {\"lr\": .001, \"momentum\": 0.875, \"dropout\": .05},\n",
    "    {\"lr\": .001, \"momentum\": 0.95, \"dropout\": .05},\n",
    "    {\"lr\": .0025, \"momentum\": 0.95, \"dropout\": .10},\n",
    "    {\"lr\": .0025, \"momentum\": 0.95, \"dropout\": .0},\n",
    "    {\"lr\": .001, \"momentum\": 0.99, \"dropout\": .10},\n",
    "    {\"lr\": .001, \"momentum\": 0.90, \"dropout\": .50},\n",
    "    {\"lr\": .005, \"momentum\": 0.90, \"dropout\": .50},\n",
    "    ]\n",
    "\n",
    "# List of 2-tuples containing (min_valid_loss, max_valid_acc) for each set \n",
    "# of evaluated parameters. Upon completion, should have same length has params.\n",
    "all_results = []\n",
    "\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. Create a side-by-side barplot with validation loss on the left and validation accuracy on the right for each parameter group. The x-axis should be the index into the params list of the given parameter set (i.e., 0, 1, 2, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Which parameter set obtained the minimum validation loss? Was this higher or lower than the best validation loss achieved in Part I?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "4. Which parameter(s) (learning rate, momentum or dropout) had the most significant impact when changed from the original model? Which direction were these changes (increase/decrease) from the model parameterization in Part I?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "5. If you were to continue working on this image classifier, what other aspects of model development might you further explore to improve model performance (2-3 sentences)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
