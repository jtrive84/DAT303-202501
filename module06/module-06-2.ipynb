{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAT303 - Module 6.2 Notebook\n",
    "---\n",
    "Name:    \n",
    "Date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is assumed you are using the module6 conda environment specified in the *module6.yaml* file downloaded from Canvas. Be sure to read all cells in this notebook. You are only to provide code in cells that contain `##### YOUR CODE HERE #####` and written responses in cells that contain `YOUR WRITTEN RESPONSE HERE`. Ensure that code cells are executed sequentially to prevent unexpected errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will fit a number of classification models and evaluate the results on the *fraud-claims.csv* dataset available on Canvas. There are three sub-sections to this assignment:\n",
    "\n",
    "- Part I: Data Preparation  \n",
    "- Part II: Classification Models\n",
    "- Part III: Evaluation  \n",
    "\n",
    "**BE SURE TO READ THE INSTRUCTIONS FOR ALL SECTIONS!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "## Part I: Data Preparation\n",
    "---\n",
    "\n",
    "You will first pre-process the dataset. You can use the example code provided [here](https://github.com/jtrive84/DMACC/blob/master/DAT303/Demos/preprocessing-pipeline-demo.ipynb) to give you an idea of how to handle imputation, scaling and one-hot encoding for categorical features. \n",
    "\n",
    "The objective is to determine whether an insurance claim is suspicious based on the available features. The target variable is \"suspicious\", where 0 represents claims that are not suspicious and 1 represents claims that are suspicious and require further investigation. For a description of the each of the columns, refer to *fraud-claims-data-dictionary.csv*. The required steps are:\n",
    "\n",
    "- Determining which features are categorical and which are continuous.\n",
    "\n",
    "- Imputing missing values (remember that imputation is handled differently for continuous and categorical features). \n",
    "\n",
    "- Scaling continuous features.\n",
    "\n",
    "- One-hot encoding categorical features.\n",
    "\n",
    "---                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.a Read *fraud-claims.csv* into a Pandas DataFrame. \n",
    "- 1.b Drop any records in which the target is missing. \n",
    "- 1.c Display the first 10 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=8, linewidth=1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "##### YOUR CODE HERE #####\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. Determine the total number of positive and negative instances in the suspicious column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "3. What proportion of the data belong to the suspicious class? Would this dataset be considered balanced or imbalanced?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. Create a barplot of the proportion (not the count) of suspicous samples by the legalrep column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Is the proportion of suspicious claims greater for individuals with or without legal representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "6. Inspect the columns of the DataFrame, and create categorical and continuous feature lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = \"suspicious\"\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "7. For each feature identified as categorical, perform a check to consolidate values into an \"OTHER\" group if they appear less than 20 times. There may not be any such cases, but add logic to handle any instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "8. Implement the preprocessing pipeline. Be sure to create train, validation and test subsets. We will use train and validation sets for modeling, but the test set will not be used until Part III to compare all models on unseen data. Print the number of rows and columns in each split. Refer to [this](https://github.com/jtrive84/DMACC/blob/master/DAT303/Demos/preprocessing-pipeline-demo.ipynb) link for an example on how to handle pre-processing in scikit-learn. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "> Note that this section should be no different from your work in *module-05-2.ipynb*. Be sure to name your datasets `dftrain`, `dfvalid` and `dftest` and your responses `ytrain`, `yvalid` and `ytest` for compatability with tests in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n",
    "\n",
    "\n",
    "\n",
    "print(f\"dftrain.shape: {dftrain.shape}\")\n",
    "print(f\"dfvalid.shape: {dfvalid.shape}\")\n",
    "print(f\"dftest.shape : {dftest.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Part II: Fitting Classification Models\n",
    "---\n",
    "In this section, you will fit 4 separate classification models, and answer any additional questions about each. In parituclar, you will fit the following:\n",
    "\n",
    "- [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier)\n",
    "- [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn-ensemble-gradientboostingclassifier)\n",
    "- Classification model of your choice (list of models available [here](https://scikit-learn.org/stable/supervised_learning.html))\n",
    "\n",
    "Follow the instructions that accompany each model. Remember that in this section, **We are only working with the training and validation sets, not the test set!**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit a `LogisticRegression` model. After fitting the model, report the accuracy, precision, recall and f1-score using the default classification threshold of .50 on the validation set. Name the resulting model `mdl1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. Using one of the approaches outlined in the classifier threshold notebook, update the classifier threshold and recalculate \n",
    "accuracy, precision, recall and f1-score on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "3. Which method did you choose to adjust the classification threshold? What is the value of the new threshold you selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. How did recall and precision change as the threshold changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Create a DataFrame consisting of the LogisticRegression coefficients along with the feature names, and sort them in decreasing absolute order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "6. Which five features have the highest absolute coefficient values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### ii. DecisionTreeClassifier\n",
    "\n",
    "1. Using `GridSearchCV`, create a parameter grid with at least three hyperparameters, and fit a `DecisionTreeClassifier` which will be identified as `mdl2`. Determine which metric to optimize against. Print the best set of parameters, and report the accuracy, precision, recall and f1-score on the validation set using the default threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. Recall that in Module 5 validation curves were discussed. For this question, you will vary the DecisionTreeClassifier's max_depth hyperparameter, and monitor how train f1-score and validation f1-score vary for each value against the default classification threshold. \n",
    "\n",
    "\n",
    "    1. For each max_depth in `np.arange(1, 51)`, do:  \n",
    "    \n",
    "        - Fit a DecisionTreeClassifier with that particular max_depth.\n",
    "        - Compute the training and validation f1-score. \n",
    "\n",
    "    2. Create a DataFrame of your results with columns max_depth, train_mse, and valid_mse. Display all rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "3. Plot the validation curve comparing train_mse and valid_mse with max_depth on the x-axis. Draw a verical black line at the value of max_depth where overfitting is starting to occur. Be sure to label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### iii. GradientBoostingClassifier\n",
    "\n",
    "1. Using `GridSearchCV`, create a parameter grid with at least four hyperparameters, and fit a `GradientBoostingClassifier` which will be identified as `mdl3`. Determine which metric to optimize against. Print the best set of parameters, and report the accuracy, precision, recall and f1-score on the validation set using the default threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. Display the ROC curve using `mdl3` predicted probabilities and validation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Display `mdl3` feature importances as a bar plot in descending order of importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. What are the top-5 features according to `mdl3`? Are any of these the same as the top-5 coefficients from `mdl1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "### iv. Model of Your Choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select a scikit-learn classification model not already covered in this notebook. Use `GridSearchCV`, with a parameter grid associated with your model which will be identified as `mdl4`. Determine which metric to optimize against. Print the best set of parameters, and report the accuracy, precision, recall and f1-score on the validation set using the default threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. Generate the precision-recall plot for `mdl4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "3. Roughly which precision-recall pair is associated with the optimal threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Part III: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell, which computes accuracy, precision, recall and f1-score on the final test set for the four models created. Recall that:\n",
    "\n",
    "- `mdl1` = LogisticRegression model\n",
    "- `mdl2` = DecisionTreeClassifier model with cross-validated hyperparmeter selection \n",
    "- `mdl3` = GradientBoostingClassifier model with cross-validated hyperparmeter selection \n",
    "- `mdl4` = Selected model of your choice with cross-validated hyperparmeter selection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run this cell as-is, no updates necessary.\n",
    "\n",
    "yhat_mdl1 = mdl1.predict(dftest)\n",
    "yhat_mdl2 = mdl2.predict(dftest)\n",
    "yhat_mdl3 = mdl3.predict(dftest)\n",
    "yhat_mdl4 = mdl4.predict(dftest)\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    {\n",
    "        \"model\": f\"{repr(mdl1)}\",\n",
    "        \"precision\": precision_score(yhat_mdl1, ytest),\n",
    "        \"recall\": recall_score(yhat_mdl1, ytest),\n",
    "        \"accuracy\": accuracy_score(yhat_mdl1, ytest),\n",
    "        \"f1\": f1_score(yhat_mdl1, ytest)\n",
    "    },\n",
    "    {\n",
    "        \"model\": f\"{repr(mdl2.estimator)}\",\n",
    "        \"precision\": precision_score(yhat_mdl2, ytest),\n",
    "        \"recall\": recall_score(yhat_mdl2, ytest),\n",
    "        \"accuracy\": accuracy_score(yhat_mdl2, ytest),\n",
    "        \"f1\": f1_score(yhat_mdl2, ytest)\n",
    "    },\n",
    "   {\n",
    "        \"model\": f\"{repr(mdl3.estimator)}\",\n",
    "        \"precision\": precision_score(yhat_mdl3, ytest),\n",
    "        \"recall\": recall_score(yhat_mdl3, ytest),\n",
    "        \"accuracy\": accuracy_score(yhat_mdl3, ytest),\n",
    "        \"f1\": f1_score(yhat_mdl3, ytest)\n",
    "    },\n",
    "    {\n",
    "        \"model\": f\"{repr(mdl4.estimator)}\",\n",
    "        \"precision\": precision_score(yhat_mdl4, ytest),\n",
    "        \"recall\": recall_score(yhat_mdl4, ytest),\n",
    "        \"accuracy\": accuracy_score(yhat_mdl4, ytest),\n",
    "        \"f1\": f1_score(yhat_mdl4, ytest)\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "pd.DataFrame().from_dict(metrics).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "1. Which model exhibited the best performance in terms of your preferred metric? Which model exhibited the worst performance in terms of your preferred metric? Why do you think the best performing model out-performed the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. Select the best model in terms of your preferred metric, and plot the confusion matrix using the default threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "3. Select the worst performing model in terms of your preferred metric and create the confusion matrix using the default classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. Compare the two confunsion matrices. How do the number of TP, TN, FP and FN differ between the two models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "5. Why did you choose the metric you selected to evaluate the classification models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
