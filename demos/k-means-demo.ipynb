{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Clustering in Python with k-Means           \n",
    "Date: 2023-06-25    \n",
    "Category: Machine Learning        \n",
    "Tags: Machine Learning, Python       \n",
    "Authors: James D. Triveri         \n",
    "Summary: Clustering in Python with k-Means     \n",
    "\n",
    "\n",
    "k-means clustering is a simple iterative clustering algorithm that partitions a \n",
    "dataset into a pre-determined number of clusters k, based on a pre-determined \n",
    "distance metric. Unlike supervised classification algorithms that have some \n",
    "notion of a target class, the objects comprising the input to k-means do not \n",
    "come with an associated target. For this reason, k-means clustering and its \n",
    "variations are considered instances of unsupervised learning techniques.\n",
    "\n",
    "In this post, details of the k-means algorithm are covered in detail. After \n",
    "laying the theoretical foundation, we walk through a Python implementation of \n",
    "k-means, and explore how to determine an optimal value for k. We'll also \n",
    "highlight a variant of k-means with an improved initialization scheme known \n",
    "as kmeans++. Finally, we examine how to leverage the clustering routines \n",
    "available in scikit-learn. \n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "Given a set of observations $(x_{1}, x_{2}, \\cdots, x_{n})$, where each \n",
    "observation is a $p$-dimensional vector, k-means attempts to partition the $n$ \n",
    "observations into $k$ clusters $C = {C_{1}, C_{2}, \\cdots, C_{k}}$ such that \n",
    "the intra-cluster sum of squares is minimized. The k-means objective function \n",
    "is defined as\n",
    "\n",
    "$$\n",
    "J = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk}||x_{n} - \\mu_{k}||^{2},\n",
    "$$\n",
    "\n",
    "which represents the sum of the squares of the distances of each data point to \n",
    "its assigned vector $\\mu_{k}$. $r_{nk}$ is given by \n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min_{j}}    \n",
    "r_{nk} =\n",
    "\\begin{cases} \n",
    "1 & \\mbox{if}  \\argmin ||x_{n}-\\mu_{j}||^{2} \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which states that the $n^{th}$ observation gets assigned to the cluster whose \n",
    "centroid is nearest.   \n",
    "Conventional k-means usually begins with random selection of k observations \n",
    "for the initial centroids. The algorithm proceeds by assigning each observation \n",
    "to its nearest centroid. Upon completion, the k centroids are updated using \n",
    "the latest cluster assignments. This is an example of an iterative refinement \n",
    "technique, which ceases only after the change in distance between consecutive \n",
    "iterations for all observations is less than some pre-determined threshold. \n",
    "\n",
    "The two stages correspond to the expectation and maximization steps of the Expectation \n",
    "Maximization algorithm. The value of $J$ is reduced after each EM cycle, so convergence \n",
    "is guaranteed, but not necessarily to the global minimum. Since k-means can be run quickly, \n",
    "various combinations of starting points and number of clusters k are run in turn to \n",
    "see how changing the observations selected for the initial k means varies the \n",
    "ultimate formation of the k clusters. \n",
    "\n",
    "## Implementation\n",
    "In what follows, we present an implementation of k-means that uses the \n",
    "conventional method of centroid intialization. In this version, the `kmeans` \n",
    "function takes as input \n",
    "data representing the observations of interest, *k* for the number of clusters \n",
    "and *max_cycles*, which limits the number of allowable EM cycles before \n",
    "iteration ceases. The function returns a dictionary containing: \n",
    "\n",
    "```\n",
    "cycles: Number of cycles required \n",
    "all_means: Progression of centroids for all cycles \n",
    "all_assign: Cluster assignments for each observation at each iteration \n",
    "all_dists: Distances from observations to each of the k centroids \n",
    "final_means: Centroids at the final iteration \n",
    "final_assign: Final cluster assignments for each observation as 1-D array \n",
    "final_dists: Final distances from observations to each of the k centroids \n",
    "total_cost: The total variance over all observations \n",
    "```\n",
    "\n",
    "The implmentation is provided below:\n",
    "\n",
    "```python\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def kmeans(data, k=3, max_cycles=25):\n",
    "    \"\"\"\n",
    "    Return means and cluster assignments for a given\n",
    "    dataset `data`.\n",
    "\n",
    "    Step I : Assignment step: Assign each observation to the cluster whose \n",
    "             mean has the least squared Euclidean distance, this is \n",
    "             intuitively the nearest mean.\n",
    "    Step II: Update step; Calculate New cluster centroids for each \n",
    "             observation.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame): \n",
    "        data = data.values\n",
    "\n",
    "    # Add index column to data.\n",
    "    tmpid = np.atleast_2d(np.arange(0, data.shape[0]))\n",
    "    X = np.concatenate([tmpid.T, data], axis=1)a\n",
    "\n",
    "    # Conventional initialization: Select k random means from  X.\n",
    "    init_indx = np.random.choice(X.shape[0], k, replace=False)\n",
    "    init_means = X[init_indx][:, 1:]\n",
    "    centroids = init_means\n",
    "    all_means, all_assign, all_dists = [], [], []\n",
    "\n",
    "    # Initialize indicator and distance matricies which indicate cluster\n",
    "    # membership for each observation.\n",
    "    init_clust_assign = np.zeros(shape=(X.shape[0], k))\n",
    "    init_clust_dists = np.zeros_like(init_clust_assign)\n",
    "\n",
    "    all_means.append(init_means)\n",
    "    all_assign.append(init_clust_assign)\n",
    "    all_dists.append(init_clust_dists)\n",
    "\n",
    "    for c in itertools.count(start=1):\n",
    "\n",
    "        clust_assign = np.zeros(shape=(X.shape[0], k))\n",
    "        clust_dists  = np.zeros_like(clust_assign)\n",
    "\n",
    "        for i in range(X.shape[0]): # Step I (Assignment Step)\n",
    "            iterobs = X[i, 1:]\n",
    "            iterdists = [distance.euclidean(j, iterobs) for j in centroids]\n",
    "            min_dist = np.min(iterdists)\n",
    "            min_indx = np.argmin(iterdists)\n",
    "            clust_assign[i, min_indx] = 1\n",
    "            clust_dists[i, min_indx]  = min_dist\n",
    "\n",
    "        # Test for convergence.\n",
    "        if np.allclose(clust_dists, all_dists[-1], atol=1e-9) or c>= max_cycles:\n",
    "            break\n",
    "\n",
    "        all_assign.append(clust_assign)\n",
    "        all_dists.append(clust_dists)\n",
    "        centroids = np.zeros_like(init_means)\n",
    "    \n",
    "        # Step II (Update Step).\n",
    "        for i in enumerate(centroids): \n",
    "            iter_k, iter_v = i[0], i[1]\n",
    "\n",
    "            # Use indicies from clust_assign to retrieve elements from X\n",
    "            # in order to calculate centroid updates.\n",
    "            iter_mean = X[np.nonzero(clust_assign[:, iter_k])][:, 1:].mean(axis=0)\n",
    "            centroids[iter_k] = iter_mean\n",
    "\n",
    "        all_means.append(centroids)\n",
    "\n",
    "    # Calculate cost function over all centroids.\n",
    "    total_cost = (all_dists[-1]**2).sum()\n",
    "\n",
    "    # Return final_assign as a 1-D array representing final cluster assignment.\n",
    "    final_assign = np.apply_along_axis(\n",
    "        lambda a: np.where(a==1)[0][0], axis=1, arr=all_assign[-1]\n",
    "        )\n",
    "\n",
    "    dresults = {\n",
    "        'k': k, 'cycles': c, 'all_means': all_means, 'all_assign': all_assign,\n",
    "        'all_dists': all_dists, 'final_means': all_means[-1], \n",
    "        'final_assign': final_assign, 'final_dists': all_dists[-1],\n",
    "        'total_cost': total_cost\n",
    "        }\n",
    "\n",
    "    return(dresults)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "At the very start of the function definition we append an identifer column \n",
    "which allows us to pair observations to their instance identifiers. \n",
    "To demonstrate, we use the dataset found [here](https://gist.githubusercontent.com/jtrive84/ea0f275a9ef010415392189e64c71fc3/raw/4bb56a71c5b597c16513c48183dde799ddf9ec51/unemp.csv), \n",
    "which contains sample mean and standard deviation of unemployment data for each of \n",
    "the 50 states: \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://gist.githubusercontent.com/jtrive84/ea0f275a9ef010415392189e64c71fc3/raw/4bb56a71c5b597c16513c48183dde799ddf9ec51/unemp.csv\")\n",
    "\n",
    "# For visualizations.\n",
    "x1 = df['mean'].values\n",
    "x2 = df['stddev'].values\n",
    "\n",
    "# Pass as input to kmeans function.\n",
    "X = df[['mean','stddev']].values\n",
    "\n",
    "results = kmeans(X, k=3)\n",
    "```\n",
    "\n",
    "\n",
    "Because `kmeans` returns the progression of centroid values and cluster \n",
    "assignments after each E-M step, we can create a visualization that \n",
    "demonstrates how assignments and centroids change with each iteration. We next \n",
    "plot cluster assignments at four iterations: \n",
    "\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Plotting various iterations of k-means cluster assignments.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Call kmeans with k=3.\n",
    "output = kmeans(X, k=3, max_cycles=25)\n",
    "am = output['all_means']\n",
    "ad = output['all_dists']\n",
    "aa = output['all_assign']\n",
    "# Plot cluster assignments on 2x2 lattice plot.\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot initial means.\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.plot(x1, x2,color=\"white\", marker=\"o\", fillstyle='none', markeredgecolor='black', markeredgewidth=.90)\n",
    "ax1.set_title(\"k-means: Random Initialization \", loc=\"left\")\n",
    "m1a, m2a = am[0][0,:]\n",
    "m1b, m2b = am[0][1,:]\n",
    "m1c, m2c = am[0][2,:]\n",
    "\n",
    "ax1.plot(m1a,m2a,color=\"#FF0080\",marker=\"D\",markersize=12,fillstyle='none',markeredgewidth=1.30)\n",
    "ax1.plot(m1b,m2b,color=\"#1A8B55\",marker=\"D\",markersize=12,fillstyle='none',markeredgewidth=1.30)\n",
    "ax1.plot(m1c,m2c,color=\"#0080FF\",marker=\"D\",markersize=12,fillstyle='none',markeredgewidth=1.30)\n",
    "ax1.plot(m1a,m2a,color=\"#FF0080\",marker=\"+\",markersize=28,fillstyle='none',markeredgewidth=1.50)\n",
    "ax1.plot(m1b,m2b,color=\"#1A8B55\",marker=\"+\",markersize=28,fillstyle='none',markeredgewidth=1.50)\n",
    "ax1.plot(m1c,m2c,color=\"#0080FF\",marker=\"+\",markersize=28,fillstyle='none',markeredgewidth=1.50)\n",
    "ax1.plot(m1a,m2a,color=\"#FF0080\",marker=\"o\")\n",
    "ax1.plot(m1b,m2b,color=\"#1A8B55\",marker=\"o\")\n",
    "ax1.plot(m1c,m2c,color=\"#0080FF\",marker=\"o\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "# 1st Assignment Step.\n",
    "ax2 = fig.add_subplot(222)\n",
    "m1a, m2a = am[0][0,:]\n",
    "m1b, m2b = am[0][1,:]\n",
    "m1c, m2c = am[0][2,:]\n",
    "\n",
    "ax2.plot(m1a,m2a,color=\"#FF0080\",marker=\"D\",markersize=12,fillstyle='none',markeredgewidth=1.30)\n",
    "ax2.plot(m1b,m2b,color=\"#1A8B55\",marker=\"D\",markersize=12,fillstyle='none',markeredgewidth=1.30)\n",
    "ax2.plot(m1c,m2c,color=\"#0080FF\",marker=\"D\",markersize=12,fillstyle='none',markeredgewidth=1.30)\n",
    "ax2.plot(m1a,m2a,color=\"#FF0080\",marker=\"+\",markersize=28,fillstyle='none',markeredgewidth=1.50)\n",
    "ax2.plot(m1b,m2b,color=\"#1A8B55\",marker=\"+\",markersize=28,fillstyle='none',markeredgewidth=1.50)\n",
    "ax2.plot(m1c,m2c,color=\"#0080FF\",marker=\"+\",markersize=28,fillstyle='none',markeredgewidth=1.50)\n",
    "\n",
    "# Retrieve points assigned to clusters.\n",
    "aa1 = aa[1]\n",
    "X0 = X[np.nonzero(aa1[:,0])][:,0:]\n",
    "x01, x02 = X0[:, 0], X0[:, 1]\n",
    "\n",
    "X1 = X[np.nonzero(aa1[:,1])][:,0:]\n",
    "x11, x12 = X1[:, 0], X1[:, 1]\n",
    "\n",
    "X2 = X[np.nonzero(aa1[:, 2])][:, 0:]\n",
    "x21, x22 = X2[:, 0], X2[:, 1]\n",
    "\n",
    "ax2.scatter(x01, x02, color=\"#FF0080\", marker=\"o\")\n",
    "ax2.scatter(x11, x12, color=\"#1A8B55\", marker=\"o\")\n",
    "ax2.scatter(x21, x22, color=\"#0080FF\", marker=\"o\")\n",
    "ax2.set_title(\"k-means: First Assignment Step\", loc=\"left\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "\n",
    "# 5th assignment step.\n",
    "ax3 = fig.add_subplot(223)\n",
    "m1a, m2a = am[4][0, :]; m1b, m2b = am[4][1, :]; m1c, m2c = am[4][2, :]\n",
    "ax3.plot(m1a, m2a, color=\"#FF0080\", marker=\"D\", markersize=12, fillstyle='none', markeredgewidth=1.30)\n",
    "ax3.plot(m1b, m2b, color=\"#1A8B55\", marker=\"D\", markersize=12, fillstyle='none', markeredgewidth=1.30)\n",
    "ax3.plot(m1c, m2c, color=\"#0080FF\", marker=\"D\", markersize=12, fillstyle='none', markeredgewidth=1.30)\n",
    "ax3.plot(m1a, m2a, color=\"#FF0080\", marker=\"+\", markersize=28, fillstyle='none', markeredgewidth=1.50)\n",
    "ax3.plot(m1b, m2b, color=\"#1A8B55\", marker=\"+\", markersize=28, fillstyle='none', markeredgewidth=1.50)\n",
    "ax3.plot(m1c, m2c, color=\"#0080FF\", marker=\"+\", markersize=28, fillstyle='none', markeredgewidth=1.50)\n",
    "\n",
    "# Retrieve points assigned to clusters.\n",
    "aa1 = aa[5]\n",
    "X0 = X[np.nonzero(aa1[:, 0])][:, 0:]\n",
    "x01, x02 = X0[:,0], X0[:, 1]\n",
    "\n",
    "X1 = X[np.nonzero(aa1[:, 1])][:, 0:]\n",
    "x11, x12 = X1[:, 0], X1[:, 1]\n",
    "\n",
    "X2 = X[np.nonzero(aa1[:, 2])][:, 0:]\n",
    "x21, x22 = X2[:, 0], X2[:, 1]\n",
    "\n",
    "ax3.scatter(x01, x02, color=\"#FF0080\", marker=\"o\")\n",
    "ax3.scatter(x11, x12, color=\"#1A8B55\", marker=\"o\")\n",
    "ax3.scatter(x21, x22, color=\"#0080FF\", marker=\"o\")\n",
    "ax3.set_title(\"k-means: Fourth Assignment Step\", loc=\"left\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "\n",
    "# Final assignment step.\n",
    "ax4 = fig.add_subplot(224)\n",
    "m1a, m2a = am[-1][0, :]; m1b, m2b = am[-1][1, :]; m1c, m2c = am[-1][2, :]\n",
    "ax4.plot(m1a, m2a, color=\"#FF0080\", marker=\"D\", markersize=12, fillstyle='none', markeredgewidth=1.30)\n",
    "ax4.plot(m1b, m2b, color=\"#1A8B55\", marker=\"D\", markersize=12, fillstyle='none', markeredgewidth=1.30)\n",
    "ax4.plot(m1c, m2c, color=\"#0080FF\", marker=\"D\", markersize=12, fillstyle='none', markeredgewidth=1.30)\n",
    "ax4.plot(m1a, m2a, color=\"#FF0080\", marker=\"+\", markersize=28, fillstyle='none', markeredgewidth=1.50)\n",
    "ax4.plot(m1b, m2b, color=\"#1A8B55\", marker=\"+\", markersize=28, fillstyle='none', markeredgewidth=1.50)\n",
    "ax4.plot(m1c, m2c, color=\"#0080FF\", marker=\"+\", markersize=28, fillstyle='none', markeredgewidth=1.50)\n",
    "\n",
    "# Retrieve points assigned to clusters.\n",
    "aa1 = aa[-1]\n",
    "X0 = X[np.nonzero(aa1[:, 0])][:, 0:]\n",
    "x01, x02 = X0[:, 0], X0[:, 1]\n",
    "\n",
    "X1 = X[np.nonzero(aa1[:, 1])][:, 0:]\n",
    "x11, x12 = X1[:, 0], X1[:, 1]\n",
    "\n",
    "X2 = X[np.nonzero(aa1[:, 2])][:, 0:]\n",
    "x21, x22 = X2[:, 0], X2[:, 1]\n",
    "\n",
    "ax4.scatter(x01, x02, color=\"#FF0080\", marker=\"o\")\n",
    "ax4.scatter(x11, x12, color=\"#1A8B55\", marker=\"o\")\n",
    "ax4.scatter(x21, x22, color=\"#0080FF\", marker=\"o\")\n",
    "ax4.set_title(\"k-means: Final Assignment Step\", loc=\"left\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=.05, hspace=0.1, left=.05, right=.95)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Which produces the following:\n",
    "\n",
    "\n",
    "![km01](https://drive.google.com/uc?id=1irDz9lz3VPfJGCgRw_KuBzd4KIblgE_t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The centroids have changed noticably when comparing the \n",
    "upper-left and the bottom-right subplots. However, we arbitrarily chose 3 as \n",
    "the number of cluster centroids. We next discuss how to determine k, which \n",
    "corresponds to the number of centroids that minimizes the cost function $J$ \n",
    "across all observations. \n",
    "\n",
    "## Determining the Optimal k\n",
    "\n",
    "A common method used in assessing the optimal number of centroids k to use \n",
    "for clustering is the elbow method, which plots the sum of squared residuals as \n",
    "a function of the number of clusters across all observations.\n",
    "If we caculate the sum of squared residuals from a single centroid model as compared with the \n",
    "same measure from the two-centroid model, it's clear that the two-centroid \n",
    "model will have a lower total variance since the larger residuals in the $k=1$ \n",
    "model will almost surely be reduced when $k=2$. However, at a certain point \n",
    "increasing k will have little impact on the intra-cluster variance, and the \n",
    "elbow method can be used to help identify this point. In the code that follows, we \n",
    "leverage our `kmeans` function and the sample dataset introduced earlier to assess total \n",
    "variance as a function of k, the number of centroids, with $1 \\leq k \\leq 10$:    \n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Demonstrating the relationship between total variance as a function of the \n",
    "number of clusters to determine the optimal number of centroids.\n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.set(context='notebook', style='darkgrid', font_scale=1)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\n",
    "    \"Total Variance as a Function of Cluster Count\", fontsize=18, \n",
    "    loc=\"left\", color=\"#FF6666\", weight=\"bold\"\n",
    "    )\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Number of Clusters\", fontsize=15, labelpad=12.5, color=\"black\")\n",
    "ax.set_ylabel(\"Total Variance\", fontsize=15, labelpad=12.5, color=\"black\")\n",
    "ax.axvline(x=3., linewidth=2., color=\"grey\")\n",
    "ax.plot(x, y, \"--\", linewidth=2.25, color=\"#FF6666\")\n",
    "ax.scatter(x, y, marker=\"o\", color=\"black\", alpha=.90)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, pad=10)\n",
    "\n",
    "# Add arrow highlighting k=3.\n",
    "ax.annotate(\"optimal k=3\", fontsize=16, xy=(3,18.85), xytext=(4,30),\n",
    "            arrowprops=dict(\n",
    "                facecolor='black', width=.6, headwidth=8, shrink=.05),)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Note the characteristic elbow shape, after which the total variance changes \n",
    "very little with the inclusion of additional centroids:  \n",
    "\n",
    "![km02](https://drive.google.com/uc?id=1ifggGbiQCiy_YR5zSyPXWA1KpTA60QzK)\n",
    "\n",
    "\n",
    "## Alternative Initialization Methods\n",
    "\n",
    "One of the shortcomings of conventional k-means is that ocassionally the \n",
    "approximation found can be arbitrarily bad with respect to the objective \n",
    "function compared to the optimal clustering. The k-means++ algorithm attempts \n",
    "to address this shortcoming by providing an alternative initialization scheme. \n",
    "In what follows, $D(x)$ denotes the shortest distance from a data point $x$ to \n",
    "the closest center $C_{i}$ already selected:\n",
    "\n",
    "1. Take one center $C_{1}$, chosen uniformly at random from the initial dataset.      \n",
    "2. Take a new center $C_{i}$, choosing $x \\in X$ with probability $\\frac {D(x)^{2}}{\\sum_{x \\in X} D(x)^{2}}$.  \n",
    "3. Repeat Step 2 until we have taken $k$ centroids in total.   \n",
    "4. Proceed with standard k-means algorithm.   \n",
    "\n",
    "\n",
    "From step 2, the expression $\\frac {D(x)^{2}}{\\sum_{x \\in X} D(x)^{2}}$ can \n",
    "be interpreted as meaning observations having a greater distance from existing \n",
    "centroids will be given a higher probability of being selected when the next \n",
    "centroid is determined. In essence, this is an attempt to spread the initial \n",
    "centroids as far as possible from one another prior to commencing k-means.    \n",
    "\n",
    "We can encapsulate the k-means++ initialization method within a function that \n",
    "takes the dataset and number of clusters k as input. A sample implementation is \n",
    "given by `kmeanspp`:\n",
    "\n",
    "```python\n",
    "def kmeanspp(X, k):\n",
    "    \"\"\"\n",
    "    Return cluster centroids using k-means++ initialization technique.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: \n",
    "        The input dataset.\n",
    "    k: \n",
    "        The number of centroids to return.\n",
    "    \"\"\"\n",
    "    # First centroids selected uniformly at random.\n",
    "    centroids = [X[np.random.choice(range(X.shape[0]), 1)[0], :]]\n",
    "\n",
    "    # Determine distance of each observation from chosen centroids.\n",
    "    for i in range(k - 1):\n",
    "        dsq = np.array(\n",
    "            [max(distance.euclidean(X[i, :], j)**2 for j in centroids)\n",
    "                for i in range(X.shape[0])]\n",
    "            )\n",
    "\n",
    "        r_indx = np.random.choice(range(X.shape[0]), 1, p=dsq/np.sum(dsq))[0]\n",
    "\n",
    "        # Ensure same index not selected multiple times.\n",
    "        if dsq[r_indx]==0:\n",
    "            while True:\n",
    "                r_indx = np.random.choice(range(X.shape[0]), 1, p=dsq/np.sum(dsq))[0]\n",
    "                if dsq[r_indx]!=0: \n",
    "                    break\n",
    "        centroids.append(X[r_indx, :])\n",
    "    return(centroids)\n",
    "```\n",
    "\n",
    "\n",
    "If we call `kmeanspp` with our original dataset and k=3, the initial \n",
    "centroids have much better spread than when using conventional initialization:  \n",
    "\n",
    "![km03](https://drive.google.com/uc?id=1ioNHukR98zY-673TUx4bd9ENEmJIO-13)\n",
    "\n",
    " \n",
    "\n",
    "## Clustering in scikit-learn\n",
    "\n",
    "The k-means clustering algorithm can be found in scikit-learn's \n",
    "`sklearn.cluster` module, where `KMeans` is one of the many available \n",
    "clustering algorithms. Information on these algorithms can be found [here](https://scikit-learn.org/stable/modules/clustering.html#clustering).\n",
    "In this section, we leverage the `KMeans` class to generate cluster \n",
    "assignments.       \n",
    "We start by scaling our variables so that no single explanatory variable \n",
    "contributes disproportionately to the overall variation in the dataset. \n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read in data as before.\n",
    "df = pd.read_csv(\"unemp.csv\")\n",
    "X = df[['mean', 'stddev']].values\n",
    "\n",
    "sclr = StandardScaler().fit(X)\n",
    "X = sclr.transform(X)\n",
    "```\n",
    "\n",
    "This transforms each explanatory variable onto a standard normal basis having \n",
    "zero mean and unit variance. Next we pass `X` to the fit method of `KMeans`: \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(X)\n",
    "\n",
    "# Get final cluster assignments.\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get final cluster centroids.\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Predict cluster assignment for new observations.\n",
    "kmeans.predict(np.atleast_2d([.515, .73337]))\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
