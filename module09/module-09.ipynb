{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAT303 - Module \\#9 Notebook\n",
    "---\n",
    "Name:    \n",
    "Date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is assumed you are using the module9 conda environment specified in the *module9.yaml* file downloaded from Canvas. Be sure to read all cells in this notebook. You are only to provide code in cells that contain `##### YOUR CODE HERE #####` and written responses in cells that contain `YOUR WRITTEN RESPONSE HERE`. Ensure that code cells are executed sequentially to prevent unexpected errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will again work with the CIFAR-10 dataset, but this time using pre-trained models in order to classify input images into one of the 10 pre-defined classes (more information on CIFAR-10 is available [here](https://dmacc.instructure.com/courses/11337/pages/the-cifar-10-dataset)). \n",
    "\n",
    "\n",
    "The tasks for this assignment are the following:\n",
    "\n",
    "* Part I: Create dataset, network definition, define train and validation functions. \n",
    "* Part II: Train `resnet34` on CIFAR-10. \n",
    "* Part III: Train `densenet121` on CIFAR-10.\n",
    "* Part IV: Train `vgg11` on CIFAR-10.\n",
    "* Part V: Compare performance of the 3 models against the final test set. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**BE SURE TO READ THE INSTRUCTIONS FOR ALL SECTIONS!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Part I.\n",
    "\n",
    "\n",
    "1. Start by loading in the CIFAR-10 dataset. The next cell loads and normalizes the images and creates `train_loader`, `valid_loader` and `test_loader` instances. Note that `test_loader` will only be used in Part V to evaluate each of the best fitting models in order to determine which model generalizes best to unseen data. Recall the differences between training, validation and test sets:\n",
    "\n",
    "    - **Training set**: Used to train the model. During this phase, the model learns the patterns and relationships within the data by adjusting its weights based on the loss calculated from its predictions. The model iteratively processes the training data, adjusting its parameters to minimize the error on this set.\n",
    "\n",
    "    - **Validation set**: Used to tune hyperparameters and evaluate the model's performance during training. It helps in assessing the model's ability to generalize to new data. During training, the model is periodically evaluated on the validation set to monitor its performance and prevent overfitting. Hyperparameter tuning, such as adjusting learning rates, batch sizes, and network architecture, is done based on validation set performance.\n",
    "\n",
    "    - **Test set**: Used to evaluate the final performance of the model after training is complete. It provides an unbiased assessment of the model's generalization capability to entirely new data. The model's performance on the test set is reported as the final measure of its accuracy, precision, recall, F1 score, or other relevant metrics. This set is only used once after the model has been trained and validated.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Execute the cell below (no additional code required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=8, linewidth=1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "\n",
    "# Number of images to process in each batch. \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# ImageNet transforms to normalize images. \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "     )\n",
    "\n",
    "\n",
    "# Download CIFAR-10 training and test dataset.\n",
    "train_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "test_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "# Use 50% of test data for validation set.\n",
    "num_test = len(test_ds)\n",
    "indices = list(range(num_test))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(.50 * num_test))\n",
    "valid_idx, test_idx = indices[split:], indices[:split]\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader(test_ds, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "print(f\"\\nNumber of training batches of size {batch_size}  : {len(train_loader)}\")\n",
    "print(f\"Number of validation batches of size {batch_size}: {len(valid_loader)}\")\n",
    "print(f\"Number of test batches of size {batch_size}      : {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. Display a batch of images to get an idea of what the training data looks like. Execute the cell below (no additional code required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get first batch from train loader. \n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 9), tight_layout=True)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"CIFAR-10 Training Images\", fontsize=12)\n",
    "ax.imshow(np.transpose(torchvision.utils.make_grid(images[:64], padding=1, normalize=True), (1, 2, 0)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Define the network architecture. Refer to the *Anatomy of a Pre-Trained Network in PyTorch* page in Canvas for a description of each component of the model. `PreTrainedNetwork` accepts a generic `pt_model` parameter, which serves as a stand-in for pre-trained PyTorch models. We disable gradient updates for all model parameters except for the final fully-connected layer. \n",
    "Note that this architecture is slightly more complicated that what is shown in the Canvas page in order to facilitate resnet, densenet and vgg pre-trained models. Execute the cell below (no additional code required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PretrainedNetwork(nn.Module):\n",
    "    def __init__(self, pt_model, dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mdl = pt_model\n",
    "\n",
    "        # Set requires_grad to False for pretrained model backbone.\n",
    "        for param in self.mdl.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Get dimension of last layer to adjust for CIFAR-10 data. The PyTorch \n",
    "        # API is not consistent w.r.t. pre-trained models, so this is necessary \n",
    "        # becuase of differences in naming conventions. \n",
    "        pt_model_name = self.mdl.__class__.__name__.strip().lower()\n",
    "\n",
    "        if pt_model_name.startswith(\"resnet\"):\n",
    "            pt_num_features = self.mdl.fc.in_features\n",
    "            self.mdl.fc = nn.Sequential(\n",
    "                nn.Linear(in_features=pt_num_features, out_features=128),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=10)\n",
    "            )\n",
    "\n",
    "        elif pt_model_name.startswith(\"densenet\"):\n",
    "            pt_num_features = self.mdl.classifier.in_features\n",
    "            self.mdl.classifier = nn.Sequential(\n",
    "                nn.Linear(in_features=pt_num_features, out_features=128),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=10)\n",
    "            )\n",
    "\n",
    "        elif pt_model_name.startswith(\"vgg\"):\n",
    "            pt_num_features = self.mdl.classifier[0].in_features\n",
    "            self.mdl.classifier = nn.Sequential(\n",
    "                nn.Linear(in_features=pt_num_features, out_features=128),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=128, out_features=10)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.mdl(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward().\n",
    "\n",
    "The code that follows shows how to load the pre-trained resnet18 model from PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. Define the loss function and optimizer. For the loss function we use cross entropy loss and stochastic gradient descent as the optimizer. Execute the cell below (no additional code required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Configuration ----------------------------------------------------------------\n",
    "\n",
    "# Number of epochs.\n",
    "n_epochs = 25\n",
    "\n",
    "# Learning rate.\n",
    "lr = 0.001\n",
    "\n",
    "# Momentum.\n",
    "momentum = .90\n",
    "\n",
    "# Dropout.\n",
    "dropout = 0.125\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Check if gpu is available. If not, use cpu. \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify loss function.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "print(f\"device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. *Define training and validation functions*. One epoch is a full pass through the training data. For example, if we had 1,000 training images and used a batch size of 50, one epoch would consist of evaluating 1000 / 50 = 20 batches of images. We usually train a network for multiple epochs. If we trained the model for 10 epochs, we would pass the full training dataset through the model 10 times.  \n",
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model's parameters. Execute the cell below (no additional code required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def epoch_trainer(epoch, data_loader, model, loss_fn, optimizer, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Execute a single training epoch. Return last batch training loss\n",
    "    and accuracy. \n",
    "    \"\"\"\n",
    "    loss, checkpoint_loss, correct, samples = 0.0, 0.0, 0, 0\n",
    "\n",
    "    # Put model in train mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over batches in data_loader. \n",
    "    for ii, (X, yactual) in enumerate(data_loader):\n",
    "\n",
    "        # Send datasets to device. \n",
    "        X, yactual = X.to(device), yactual.to(device)\n",
    "\n",
    "        # Zero out parameter gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get model predictions (forward pass). \n",
    "        ypred = model(X)\n",
    "\n",
    "        # Compute loss. \n",
    "        loss_ii = loss_fn(ypred, yactual)\n",
    "\n",
    "        # Backpropagation and optimizer step. \n",
    "        loss_ii.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running_loss.\n",
    "        loss+=loss_ii.item()\n",
    "        correct+=(ypred.argmax(dim=1)==yactual).type(torch.float).sum().item()\n",
    "        samples+=yactual.size(dim=0)\n",
    "\n",
    "        # Print running_loss for every 100 mini-batches.\n",
    "        if ii % 250 == 0:\n",
    "            checkpoint_acc = correct / samples\n",
    "            checkpoint_loss = loss / 250\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\t+ [train][epoch={epoch}, batch={ii}] loss = {checkpoint_loss:,.5f}, acc = {checkpoint_acc:.5f}.\")\n",
    "            \n",
    "            loss, correct, samples = 0.0, 0, 0\n",
    "\n",
    "    return checkpoint_loss, checkpoint_acc\n",
    "        \n",
    "\n",
    "\n",
    "def epoch_validator(data_loader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Execute a single validation epoch. Return average validation loss\n",
    "    and accuracy.\n",
    "    \"\"\"\n",
    "    valid_loss, correct = 0.0, 0\n",
    "\n",
    "    # Put model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for ii, (X, yactual) in enumerate(data_loader, start=1):\n",
    "\n",
    "            # Send dataset and target to device. \n",
    "            X, yactual = X.to(device), yactual.to(device)\n",
    "\n",
    "            # Get model predictions. \n",
    "            ypred = model(X)\n",
    "\n",
    "            # Compute loss and update valid_loss.\n",
    "            valid_loss+=loss_fn(ypred, yactual).item()\n",
    "\n",
    "            # Count number of correct class predictions.\n",
    "            correct+=(ypred.argmax(dim=1)==yactual).type(torch.float).sum().item()\n",
    "\n",
    "    loss, acc = valid_loss / ii, correct / len(data_loader.dataset)\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Part II.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Initialize an instance of `PretrainedNetwork` that accepts the `resnet34` pre-trained model. Identify this model as `mdl1`. Be sure to pass in `dropout`, which is set to .125 above. Put model on `device` (i.e. append `.to(device)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. In a sentence or two, describe the defining characteristics of ResNet models. What does the '34' represent in 'resnet34'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Print the total number of parameters in `mdl1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "4. Train `mdl1` for specified number of epochs. `results1` is a list of 4-tuples consisting of train loss, train accuracy, validation loss and validation accuracy. Within the loop, whichever epoch achieves the minimum validation loss, the model at that time is written to disk as *best-mdl1.pth*. Execute the cell below (no additional code required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results1 = []\n",
    "best_loss = np.Inf\n",
    "\n",
    "# Specify optimizer. Decay learning rate by a factor of .10 every 5 epochs.\n",
    "optimizer = optim.SGD(mdl1.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.10)\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    tloss, tacc = epoch_trainer(\n",
    "        epoch=epoch, data_loader=train_loader, model=mdl1, loss_fn=loss_fn, \n",
    "        optimizer=optimizer, device=device\n",
    "        )\n",
    "    \n",
    "    vloss, vacc = epoch_validator(\n",
    "        data_loader=valid_loader, model=mdl1, loss_fn=loss_fn, \n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    if vloss < best_loss:\n",
    "        best_loss = vloss\n",
    "        torch.save(mdl1, \"best-mdl1.pth\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"mdl1 [epoch={epoch}-of-{n_epochs}]: tloss={tloss:.5f}, tacc={tacc:.5f}, vloss={vloss:.5f}, vacc={vacc:.5f}.\")\n",
    "\n",
    "    # Append metrics to results.\n",
    "    results1.append((tloss, tacc, vloss, vacc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Which epoch achieved the minimum validation loss for `mdl1`? The maximum validation accuracy? What are the values of the minimum validation loss and maximum validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Part III.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Initialize and instance of `PretrainedNetwork` that accepts the `densenet121` pre-trained model. Identify this model as `mdl2`. Be sure to pass in `dropout`, which is set to .125 above. Put model on device (i.e. call `.to(device)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "2. In a sentence or two, describe the defining characteristics of DenseNet models. What does the '121' represent in 'densenet121'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Print the total number of parameters in `mdl2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "4. Train `mdl2` for specified number of epochs. `results2` is a list of 4-tuples consisting of train loss, train accuracy, validation loss and validation accuracy. Within the loop, whichever epoch achieves the minimum validation loss, the model at that time is written to disk as *best-mdl2.pth*. Copy the code from Part I, replacing instances of `mdl1` with `mdl2` and execute the cell to commence training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results2 = []\n",
    "best_loss = np.Inf\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Which epoch achieved the minimum validation loss for `mdl2`? The maximum validation accuracy? What are the values of the minimum validation loss and maximum validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Part IV.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Initialize and instance of `PretrainedNetwork` that accepts the `vgg11` pre-trained model. Identify this model as `mdl3`. Be sure to pass in `dropout`, which is set to .125 above. Put model on `device` (i.e. call `.to(device)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. In a sentence or two, describe the defining characteristics of VGG models. What does the '11' represent in 'vgg11'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "3. Print the total number of parameters in `mdl3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "4. Train `mdl3` for specified number of epochs. `results3` is a list of 4-tuples consisting of train loss, train accuracy, validation loss and validation accuracy. Within the loop, whichever epoch achieves the minimum validation loss, the model at that time is written to disk as *best-mdl3.pth*. Copy the code from Part I, replacing instances of `mdl1` with `mdl3` and execute the cell to commence training.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results3 = []\n",
    "best_loss = np.Inf\n",
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Which epoch achieved the minimum validation loss for `mdl3`? The maximum validation accuracy? What are the values of the minimum validation loss and maximum validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## Part V.\n",
    "\n",
    "\n",
    "1. Create a side-by-side plot with validation loss on the left and validation accuracy on the right for mdl1, mdl2 and mdl3 (three separate line plots on each facet). These metrics should be available in `results1`, `results2` and `results3`. \n",
    "For the validation accuracy plot, include a horizontal dashed line at 10% highlighting the accuracy of the null model. \n",
    "Be sure to include a legend and label you axes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. Which model achieved minimum validation loss? Did the same model achieve maximum validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "3. Using `torch.load`, load the best performing model objects from *best-mdl1.pth*, *best-mdl2.pth* and *best-mdl3.pth*. Bind each to `mdl1`, `mdl2` and `mdl3` respectively. Be sure to append `.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "4. Using the `epoch_validator` function defined earlier, compute the final loss and accuracy on the test set for `mdl1`, `mdl2` and `mdl3`. Be sure to pass `test_loader` into `epoch_validator`. Print test loss and accuracy for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Which model acheived the highest accuracy on the test set? Does this correspond to the model that had the most parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "YOUR WRITTEN RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "6. In the next cell, `X` represents a batch of 32 images and `y` the corresponding labels (see `classes` list below: Within `y`, 0 represents \"plane\", 1 represents \"car\", etc.).   \n",
    "When `X` is passed into the model, the result will be a 32x10 tensor. The predicted class will be the index with the maximum value for each row. You can use `torch.argmax` to find the index with the maximum value per row (result should be a tensor of size 32. Using your best performing model identified in the previous question, determine the class predictions for the batch of 32 images and compare it to the actual class. How many images did your model correctly predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CIFAR-10 label descriptions.\n",
    "classes = [\n",
    "    \"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "    ]\n",
    "\n",
    "X, y = next(iter(test_loader))\n",
    "X, y = X.to(device), y.to(device)\n",
    "\n",
    "\n",
    "##### YOUR CODE HERE ##### \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
